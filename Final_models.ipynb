{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8ae6a139-9580-4d4a-811a-ca7cee6af905",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Lambda, Concatenate, BatchNormalization\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, UpSampling2D, Reshape, Conv2DTranspose\n",
    "from tensorflow.keras.layers import LSTM, RepeatVector, TimeDistributed, concatenate\n",
    "from tensorflow.keras.regularizers import l1_l2\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.preprocessing import MinMaxScaler, RobustScaler, PowerTransformer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import precision_recall_curve, average_precision_score, roc_curve, auc\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from datetime import datetime\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "acd5bb4d-24b2-406f-b947-65a27daec0b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up directories for storing results\n",
    "current_time = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "results_folder = f\"thermal_analysis_{current_time}\"\n",
    "os.makedirs(results_folder, exist_ok=True)\n",
    "\n",
    "models_folder = os.path.join(results_folder, \"saved_models\")\n",
    "os.makedirs(models_folder, exist_ok=True)\n",
    "\n",
    "graphs_folder = os.path.join(results_folder, \"visuals\")\n",
    "os.makedirs(graphs_folder, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f909237c-447e-4f24-bef2-0bc8e11de3d0",
   "metadata": {},
   "source": [
    "# DATA LOADING AND PREPROCESSING "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b18b2368-08e9-4696-af8c-a2fb41d9f9ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv(\"data.csv\")\n",
    "dataset = dataset.drop(columns=[\"file_name\"])\n",
    "\n",
    "# Normalize data\n",
    "scaler = MinMaxScaler()\n",
    "normalized_data = scaler.fit_transform(dataset)\n",
    "\n",
    "# Train-test split\n",
    "train_data, test_data = train_test_split(normalized_data, test_size=0.2, random_state=42)\n",
    "feature_count = normalized_data.shape[1]\n",
    "\n",
    "# Optional transformations\n",
    "apply_power_transform = False\n",
    "if apply_power_transform:\n",
    "    transformer = PowerTransformer(method='yeo-johnson')\n",
    "    train_data = transformer.fit_transform(train_data)\n",
    "    test_data = transformer.transform(test_data)\n",
    "\n",
    "apply_robust_scaling = False\n",
    "if apply_robust_scaling:\n",
    "    robust_scaler = RobustScaler()\n",
    "    train_data = robust_scaler.fit_transform(train_data)\n",
    "    test_data = robust_scaler.transform(test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2ee7198-e4e9-49eb-9460-8929420660a0",
   "metadata": {},
   "source": [
    "# UTILITY FUNCTIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6e1ad65c-22bf-4117-add0-2fae792207ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_reconstruction_error(model, data):\n",
    "    reconstructions = model.predict(data)\n",
    "    mse = np.mean(np.square(data - reconstructions), axis=1)\n",
    "    return mse, reconstructions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9371c8a-d11d-499a-aed3-f7f3e54951af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomaly_detector(model, X_train, X_test, model_name, plot_dir):\n",
    "    # Calculate reconstruction errors\n",
    "    train_error, train_reconstructions = calculate_reconstruction_error(model, X_train)\n",
    "    test_error, test_reconstructions = calculate_reconstruction_error(model, X_test)\n",
    "\n",
    "    # Set threshold using the 95th percentile of the training data error\n",
    "    threshold = np.percentile(train_error, 95)\n",
    "\n",
    "    # Simulate anomalies for demonstration purposes\n",
    "    predicted_anomalies = test_error > threshold\n",
    "    simulated_true_anomalies = test_error > np.percentile(test_error, 95)\n",
    "\n",
    "    # Plot reconstruction error distributions\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.hist(train_error, bins=50, alpha=0.5, label='Training Data')\n",
    "    plt.hist(test_error, bins=50, alpha=0.5, label='Test Data')\n",
    "    plt.axvline(threshold, color='r', linestyle='--', label=f'Threshold ({threshold:.4f})')\n",
    "    plt.title(f'Reconstruction Error Distribution - {model_name}')\n",
    "    plt.xlabel('Reconstruction Error')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Plot ROC curve\n",
    "    fpr, tpr, _ = roc_curve(simulated_true_anomalies, test_error)\n",
    "    roc_auc = auc(fpr, tpr)\n",
    "    plt.figure(figsize=(8, 8))\n",
    "    plt.plot(fpr, tpr, color='darkorange', lw=2, label=f'ROC curve (area = {roc_auc:.2f})')\n",
    "    plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')\n",
    "    plt.xlim([0.0, 1.0])\n",
    "    plt.ylim([0.0, 1.05])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate')\n",
    "    plt.title(f'Receiver Operating Characteristic - {model_name}')\n",
    "    plt.legend(loc=\"lower right\")\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Visualize reconstruction examples\n",
    "    n_examples = min(5, len(X_test))\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    for i in range(n_examples):\n",
    "        plt.subplot(2, n_examples, i+1)\n",
    "        plt.plot(X_test[i])\n",
    "        plt.title(f\"Original {i+1}\")\n",
    "        plt.grid(True)\n",
    "        plt.subplot(2, n_examples, i+n_examples+1)\n",
    "        plt.plot(test_reconstructions[i])\n",
    "        plt.title(f\"Reconstructed {i+1}\")\n",
    "        plt.grid(True)\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    return train_error, test_error, threshold, predicted_anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc368198-0238-40de-ba7b-69fed2738326",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_and_evaluate_model(model, model_name, X_train, X_test, epochs=100, batch_size=32):\n",
    "    print(f\"Training {model_name}...\")\n",
    "    early_stopping = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)\n",
    "    model_checkpoint = ModelCheckpoint(\n",
    "        os.path.join(model_dir, f'{model_name}.keras'),\n",
    "        monitor='val_loss', \n",
    "        save_best_only=True\n",
    "    )\n",
    "    reduce_lr = ReduceLROnPlateau(monitor='val_loss', factor=0.5, patience=5, min_lr=1e-6)\n",
    "\n",
    "    start_time = time.time()\n",
    "    history = model.fit(\n",
    "        X_train, X_train,\n",
    "        epochs=epochs,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        validation_data=(X_test, X_test),\n",
    "        callbacks=[early_stopping, model_checkpoint, reduce_lr],\n",
    "        verbose=1\n",
    "    )\n",
    "    training_time = time.time() - start_time\n",
    "    print(f\"{model_name} training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "    # Plot training history\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(history.history['loss'], label='Training Loss')\n",
    "    plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "    plt.title(f'{model_name} - Training History')\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "    plt.close()\n",
    "\n",
    "    # Evaluate model\n",
    "    train_error, test_error, threshold, predicted_anomalies = evaluate_anomaly_detector(\n",
    "        model, X_train, X_test, model_name, plot_dir\n",
    "    )\n",
    "\n",
    "    results = {\n",
    "        'model_name': model_name,\n",
    "        'training_time': training_time,\n",
    "        'mean_train_error': np.mean(train_error),\n",
    "        'mean_test_error': np.mean(test_error),\n",
    "        'threshold': threshold,\n",
    "        'anomalies_detected': np.sum(predicted_anomalies)\n",
    "    }\n",
    "    return results, train_error, test_error, predicted_anomalies"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e33e0da0-a1cf-4e88-b136-28823ea0d531",
   "metadata": {},
   "source": [
    "# MODEL DEFINITIONS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e681a898-c451-4889-98cf-9a5b29e90c11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_advanced_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    x = Dense(128, activation='elu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(input_layer)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(64, activation='elu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "\n",
    "    skip_connection = Dense(32, activation='linear')(x)\n",
    "    x = Dense(32, activation='elu')(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Concatenate()([x, skip_connection])\n",
    "\n",
    "    attention_weights = Dense(16, activation='softmax', name='attention')(x)\n",
    "    bottleneck = Dense(16, activation='elu', name='bottleneck')(x)\n",
    "    encoded = Lambda(lambda inputs: inputs[0] * inputs[1])([bottleneck, attention_weights])\n",
    "\n",
    "    x = Dense(32, activation='elu')(encoded)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dense(64, activation='elu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    x = Dropout(0.2)(x)\n",
    "    x = Dense(128, activation='elu', kernel_regularizer=l1_l2(l1=1e-5, l2=1e-4))(x)\n",
    "    x = BatchNormalization()(x)\n",
    "    decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    def thermal_gradient_loss(y_true, y_pred):\n",
    "        mse = tf.reduce_mean(tf.square(y_true - y_pred), axis=-1)\n",
    "        y_true_grad = tf.abs(y_true[:, 1:] - y_true[:, :-1])\n",
    "        y_pred_grad = tf.abs(y_pred[:, 1:] - y_pred[:, :-1])\n",
    "        gradient_loss = tf.reduce_mean(tf.square(y_true_grad - y_pred_grad), axis=-1)\n",
    "        return mse + 0.5 * gradient_loss\n",
    "\n",
    "    optimizer = Adam(learning_rate=0.001, clipnorm=1.0)\n",
    "    autoencoder.compile(optimizer=optimizer, loss=thermal_gradient_loss)\n",
    "    return autoencode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88551d01-1325-4f43-bacb-36274396912e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_temporal_autoencoder(input_dim):\n",
    "    input_layer = Input(shape=(input_dim,))\n",
    "    seq_length = 10\n",
    "    feature_dim = input_dim // seq_length\n",
    "\n",
    "    if input_dim % seq_length == 0:\n",
    "        reshaped = Reshape((seq_length, feature_dim))(input_layer)\n",
    "        x = LSTM(64, return_sequences=True)(reshaped)\n",
    "        x = LSTM(32, return_sequences=False)(x)\n",
    "        encoded = Dense(16, activation='relu')(x)\n",
    "        x = RepeatVector(seq_length)(encoded)\n",
    "        x = LSTM(32, return_sequences=True)(x)\n",
    "        x = LSTM(64, return_sequences=True)(x)\n",
    "        x = TimeDistributed(Dense(feature_dim))(x)\n",
    "        decoded = Reshape((input_dim,))(x)\n",
    "    else:\n",
    "        x = Dense(128, activation='relu')(input_layer)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(64, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        encoded = Dense(32, activation='relu')(x)\n",
    "        x = Dense(64, activation='relu')(encoded)\n",
    "        x = BatchNormalization()(x)\n",
    "        x = Dense(128, activation='relu')(x)\n",
    "        x = BatchNormalization()(x)\n",
    "        decoded = Dense(input_dim, activation='sigmoid')(x)\n",
    "\n",
    "    autoencoder = Model(input_layer, decoded)\n",
    "    autoencoder.compile(optimizer='adam', loss='mse')\n",
    "    return autoencoder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cc067aa-6873-4a5a-8e21-7e50bfd36b26",
   "metadata": {},
   "source": [
    "#  MAIN EXECUTION"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3e33db9-8ae5-4be3-93af-0095d883c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_results = []\n",
    "anomaly_scores = {}\n",
    "\n",
    "# Train and evaluate Model 1: Advanced Autoencoder\n",
    "model1 = build_advanced_autoencoder(input_dim)\n",
    "model1_results, train_error1, test_error1, anomalies1 = train_and_evaluate_model(\n",
    "    model1, \"Advanced_Thermal_Autoencoder\", X_train, X_test, epochs=100, batch_size=32\n",
    ")\n",
    "all_results.append(model1_results)\n",
    "anomaly_scores[\"Advanced_Thermal_Autoencoder\"] = test_error1\n",
    "\n",
    "# Train and evaluate Model 3: Temporal Autoencoder\n",
    "model3 = build_temporal_autoencoder(input_dim)\n",
    "model3_results, train_error3, test_error3, anomalies3 = train_and_evaluate_model(\n",
    "    model3, \"Temporal_Autoencoder\", X_train, X_test, epochs=100, batch_size=32\n",
    ")\n",
    "all_results.append(model3_results)\n",
    "anomaly_scores[\"Temporal_Autoencoder\"] = test_error3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9662c6b7-b812-43d5-872d-d6e468a9a154",
   "metadata": {},
   "source": [
    "# ENSEMBLE METHOD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ef05f880-609f-482b-8705-c6b0ed0e4845",
   "metadata": {},
   "outputs": [],
   "source": [
    "ensemble_scores = np.zeros_like(test_error1)\n",
    "\n",
    "for model_name, scores in anomaly_scores.items():\n",
    "    normalized_scores = (scores - np.min(scores)) / (np.max(scores) - np.min(scores))\n",
    "    ensemble_scores += normalized_scores\n",
    "\n",
    "# Average the ensemble scores\n",
    "ensemble_scores /= len(anomaly_scores)\n",
    "\n",
    "# Determine the ensemble anomaly threshold (95th percentile)\n",
    "ensemble_threshold = np.percentile(ensemble_scores, 95)\n",
    "ensemble_anomalies = ensemble_scores > ensemble_threshold\n",
    "\n",
    "# Plot Ensemble Anomaly Score Distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(ensemble_scores, bins=50, alpha=0.7)\n",
    "plt.axvline(ensemble_threshold, color='r', linestyle='--', label=f'Threshold ({ensemble_threshold:.4f})')\n",
    "plt.title('Ensemble Anomaly Score Distribution')\n",
    "plt.xlabel('Anomaly Score')\n",
    "plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e05e39fb-76cc-4e7e-9b2e-ac62d2e0394b",
   "metadata": {},
   "source": [
    "# RESULTS "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de7a4454-c400-45a5-beb2-06aff15ad85c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                          Model  Mean Anomaly Score  Max Anomaly Score  \\\n",
      "0  Advanced_Thermal_Autoencoder            0.000549           0.033328   \n",
      "1          Temporal_Autoencoder            0.000613           0.051827   \n",
      "2                      Ensemble            0.014089           0.765950   \n",
      "\n",
      "   Anomalies Detected  \n",
      "0                  18  \n",
      "1                  18  \n",
      "2                  18  \n",
      "All results saved to thermal_anomaly_results_20250324_113434\n",
      "Analysis complete!\n"
     ]
    }
   ],
   "source": [
    "results_df = pd.DataFrame(all_results)\n",
    "results_df.to_csv(os.path.join(output_dir, 'model_performance_summary2.csv'), index=False)\n",
    "\n",
    "comparison_summary = {\n",
    "    'Model': list(anomaly_scores.keys()) + ['Ensemble'],\n",
    "    'Mean Anomaly Score': [np.mean(scores) for scores in anomaly_scores.values()] + [np.mean(ensemble_scores)],\n",
    "    'Max Anomaly Score': [np.max(scores) for scores in anomaly_scores.values()] + [np.max(ensemble_scores)],\n",
    "    'Anomalies Detected': [np.sum(scores > np.percentile(scores, 95)) for scores in anomaly_scores.values()] + [np.sum(ensemble_anomalies)]\n",
    "}\n",
    "comparison_df = pd.DataFrame(comparison_summary)\n",
    "comparison_df.to_csv(os.path.join(output_dir, 'model_comparison2.csv'), index=False)\n",
    "print(comparison_df)\n",
    "\n",
    "anomaly_results = pd.DataFrame({\n",
    "    'Advanced_Thermal_Autoencoder': test_error1,\n",
    "    'Temporal_Autoencoder': test_error3,\n",
    "    'Ensemble': ensemble_scores\n",
    "})\n",
    "anomaly_results.to_csv(os.path.join(output_dir, 'anomaly_scores2.csv'), index=False)\n",
    "print(f\"All results saved to {output_dir}\")\n",
    "print(\"Analysis complete!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "946c769e-c30d-4aec-a49a-4819d14732bb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
